{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "27111093",
      "metadata": {
        "id": "27111093"
      },
      "source": [
        "# SC_ASS1: FP16 vs INT8 vs INT8_VIT Benchmark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c56f4127",
      "metadata": {
        "id": "c56f4127"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers accelerate bitsandbytes datasets pyarrow pillow seaborn matplotlib tqdm codecarbon\n",
        "!pip install -q git+https://github.com/salaniz/pycocoevalcap.git@master"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b2317d5",
      "metadata": {
        "id": "4b2317d5"
      },
      "source": [
        "## 1 — Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "849de064",
      "metadata": {
        "id": "849de064"
      },
      "outputs": [],
      "source": [
        "import yaml, json, os\n",
        "from datetime import datetime\n",
        "\n",
        "EXP = {\n",
        "    \"experiment_name\": \"VLM_Quantization\",\n",
        "    \"model_id\": \"Qwen/Qwen2.5-VL-3B-Instruct\",\n",
        "    \"quantizer\": \"bitsandbytes_linear8bitlt\",\n",
        "    \"precision\": {\n",
        "        \"language_model\": \"fp16\"\n",
        "    },\n",
        "    \"dataset\": {\n",
        "        \"hf_parquet_prefix\": \"hf://datasets/phiyodr/coco2017/\",\n",
        "        \"split_file\": \"data/validation-00000-of-00001-e3c37e369512a3aa.parquet\",\n",
        "        \"limit\": 5000\n",
        "    },\n",
        "    \"seed\": 42,\n",
        "    \"inference\": {\n",
        "        \"max_new_tokens\": 40,\n",
        "        \"img_size\": 1024,\n",
        "        \"batch_size\": 1000,\n",
        "    }\n",
        "}\n",
        "\n",
        "os.makedirs('results', exist_ok=True)\n",
        "config_path = f'results/experiment_config_{datetime.utcnow().strftime(\"%Y%m%dT%H%M%SZ\")}.yaml'\n",
        "with open(config_path, 'w') as f:\n",
        "    yaml.safe_dump(EXP, f)\n",
        "print('Saved experiment config to', config_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9889ede0",
      "metadata": {
        "id": "9889ede0"
      },
      "source": [
        "## 2 — Utilities (image loader, timing, VRAM helpers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "825cbe5b",
      "metadata": {
        "id": "825cbe5b"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import torch\n",
        "import random\n",
        "import requests\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from io import BytesIO\n",
        "import os\n",
        "\n",
        "random.seed(EXP['seed'])\n",
        "torch.manual_seed(EXP['seed'])\n",
        "\n",
        "def load_image(path, max_size=1024):\n",
        "    response = requests.get(path)\n",
        "    img = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "    w, h = img.size\n",
        "    scale = min(max_size / w, max_size / h, 1.0)  # don't upsample\n",
        "    new_w, new_h = int(w*scale), int(h*scale)\n",
        "    if scale < 1.0:\n",
        "        img = img.resize((new_w, new_h))\n",
        "    return img\n",
        "\n",
        "def reset_cuda_stats():\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "def get_peak_vram_mib():\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.cuda.max_memory_allocated() / 1024**2\n",
        "    return 0.0\n",
        "\n",
        "def get_caption(result):\n",
        "    if not result:\n",
        "        return None\n",
        "\n",
        "    generated = result[0].get(\"generated_text\", [])\n",
        "    if not generated:\n",
        "        return None\n",
        "\n",
        "    # look for the first assistant entry\n",
        "    for entry in generated:\n",
        "        if entry.get(\"role\") == \"assistant\":\n",
        "            content = entry.get(\"content\")\n",
        "            if isinstance(content, str):\n",
        "                # direct string\n",
        "                return content.strip()\n",
        "            elif isinstance(content, list):\n",
        "                # list of dicts\n",
        "                for c in content:\n",
        "                    if isinstance(c, dict) and c.get(\"type\") == \"text\" and \"text\" in c:\n",
        "                        return c[\"text\"].strip()\n",
        "    return None\n",
        "\n",
        "\n",
        "def get_model_size(model):\n",
        "    total_bytes = 0\n",
        "    for param in model.parameters():\n",
        "        total_bytes += param.numel() * param.element_size()  # num elements × bytes per element\n",
        "    return total_bytes / (1024**2)  # convert to MB\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60beaead",
      "metadata": {
        "id": "60beaead"
      },
      "source": [
        "## 3 — Dataset loader\n",
        "This cell shows how to load your parquet split from HF or local path."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fee8ca4f",
      "metadata": {
        "id": "fee8ca4f"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load parquet (hf:// or local)\n",
        "parquet_path = EXP['dataset']['hf_parquet_prefix'] + EXP['dataset']['split_file']\n",
        "print('Loading dataset from', parquet_path)\n",
        "try:\n",
        "    ds = load_dataset('parquet', data_files=parquet_path, split='train')\n",
        "except Exception as e:\n",
        "    print('Failed to load via datasets.load_dataset:', e)\n",
        "    print('Try providing a local path to the parquet file instead.')\n",
        "    raise\n",
        "\n",
        "print('Dataset length:', len(ds))\n",
        "print('Columns:', ds.column_names)\n",
        "\n",
        "print('Example row:', ds[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a366b29d",
      "metadata": {
        "id": "a366b29d"
      },
      "source": [
        "## 4 — Model builders and quantization (bitsandbytes vision-only)\n",
        "We load the full model in FP16 and then replace linear layers in the vision encoder with `bnb.nn.Linear8bitLt`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80c405a2",
      "metadata": {
        "id": "80c405a2"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline, BitsAndBytesConfig, AutoModelForImageTextToText, AutoProcessor\n",
        "import bitsandbytes as bnb\n",
        "import torch.nn as nn\n",
        "\n",
        "MODEL_ID = EXP['model_id']\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print('Loading models on', device)\n",
        "\n",
        "def build_fp16_model(model_id=MODEL_ID):\n",
        "    processor = AutoProcessor.from_pretrained(model_id)\n",
        "    model = AutoModelForImageTextToText.from_pretrained(\n",
        "        model_id,\n",
        "        device_map=\"auto\",\n",
        "        dtype=torch.bfloat16,\n",
        "    )\n",
        "\n",
        "    pipe = pipeline(\n",
        "        \"image-text-to-text\",\n",
        "        model=model,\n",
        "        processor=processor,\n",
        "        device_map=\"auto\",\n",
        "    )\n",
        "\n",
        "    return pipe\n",
        "\n",
        "def build_int8_model(model_id=MODEL_ID):\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_8bit=True,\n",
        "    )\n",
        "\n",
        "    processor = AutoProcessor.from_pretrained(model_id)\n",
        "    model = AutoModelForImageTextToText.from_pretrained(\n",
        "        model_id,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\",\n",
        "        dtype=torch.float16,\n",
        "    )\n",
        "\n",
        "    pipe = pipeline(\n",
        "        \"image-text-to-text\",\n",
        "        model=model,\n",
        "        processor=processor,\n",
        "        device_map=\"auto\",\n",
        "    )\n",
        "\n",
        "    return pipe\n",
        "\n",
        "def quantize_visual_mlp_to_8bit(model):\n",
        "    quantized_count = 0\n",
        "    model_dtype = next(model.parameters()).dtype\n",
        "\n",
        "    for name, module in list(model.named_modules()):\n",
        "        # Match layers inside the vision tower MLPs\n",
        "        if (\n",
        "            \"vision_tower\" in name or \"visual\" in name\n",
        "        ) and isinstance(module, nn.Linear):\n",
        "\n",
        "            parent_name = name.split(\".\")[:-1]\n",
        "            attr_name = name.split(\".\")[-1]\n",
        "\n",
        "            # Get parent module\n",
        "            parent = model\n",
        "            for p in parent_name:\n",
        "                parent = getattr(parent, p)\n",
        "\n",
        "            # Create new 8-bit layer\n",
        "            new_linear = bnb.nn.Linear8bitLt(\n",
        "                module.in_features,\n",
        "                module.out_features,\n",
        "                bias=module.bias is not None,\n",
        "                has_fp16_weights=True,        # keep weights compressed but activations in fp16/bf16\n",
        "            ).to(module.weight.device)\n",
        "\n",
        "            # Copy weights and bias\n",
        "            new_linear.weight.data.copy_(module.weight.data.to(torch.float16))\n",
        "            if module.bias is not None:\n",
        "                new_linear.bias.data.copy_(module.bias.data.to(torch.float16))\n",
        "\n",
        "            # Replace layer\n",
        "            setattr(parent, attr_name, new_linear)\n",
        "            quantized_count += 1\n",
        "\n",
        "    print(f\"Quantized {quantized_count} Linear layers in visual tower to 8-bit using bitsandbytes.\")\n",
        "    return model\n",
        "\n",
        "def build_vit_int8_model(model_id):\n",
        "    processor = AutoProcessor.from_pretrained(model_id)\n",
        "    model = AutoModelForImageTextToText.from_pretrained(\n",
        "        model_id,\n",
        "        dtype=torch.float16,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "\n",
        "    model = quantize_visual_mlp_to_8bit(model)\n",
        "\n",
        "    pipe = pipeline(\n",
        "        \"image-text-to-text\",\n",
        "        model=model,\n",
        "        processor=processor,\n",
        "    )\n",
        "\n",
        "    return pipe\n",
        "\n",
        "print('Model builder ready')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48d59ba6",
      "metadata": {
        "id": "48d59ba6"
      },
      "source": [
        "## 5 — Inference helper (processor + model.generate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ccdd3f9",
      "metadata": {
        "id": "3ccdd3f9"
      },
      "outputs": [],
      "source": [
        "def generate_caption_with_model(pipe, image, prompt_text=\"Give a short caption.\", max_new_tokens=24):\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"image\", \"image\": image},\n",
        "                {\"type\": \"text\", \"text\": prompt_text},\n",
        "            ],\n",
        "        }\n",
        "    ]\n",
        "    with torch.inference_mode():\n",
        "        output = pipe(messages, max_new_tokens=max_new_tokens)\n",
        "\n",
        "    caption = get_caption(output)\n",
        "    return caption\n",
        "\n",
        "print('Inference helper ready.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2c3a7e4",
      "metadata": {
        "id": "f2c3a7e4"
      },
      "source": [
        "## 6 — Evaluation loop (runs over dataset, records latency, peak VRAM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac40463d",
      "metadata": {
        "id": "ac40463d"
      },
      "outputs": [],
      "source": [
        "def evaluate_full(dataset, pipe, img_field='coco_url', refs_field='captions',\n",
        "                  max_items=None, img_size=1024, max_new_tokens=24):\n",
        "    results = []\n",
        "    length = len(dataset)\n",
        "    n = length if max_items is None else min(length, max_items)\n",
        "    print(f\"Evaluating {n} images...\")\n",
        "\n",
        "    for i in tqdm(range(n), desc='Eval'):\n",
        "        row = dataset[i]\n",
        "\n",
        "        # Load image\n",
        "        image_input = row.get(img_field) if isinstance(row, dict) else row[img_field]\n",
        "        img = load_image(image_input, img_size)\n",
        "\n",
        "        # Load references\n",
        "        refs = row.get(refs_field) if isinstance(row, dict) else row[refs_field]\n",
        "\n",
        "        # Reset GPU stats\n",
        "        reset_cuda_stats()\n",
        "        t0 = time.time()\n",
        "\n",
        "        # Generate caption safely\n",
        "        try:\n",
        "            pred = generate_caption_with_model(pipe, img, max_new_tokens=max_new_tokens)\n",
        "        except Exception as e:\n",
        "            print(f\"Skipping image {i} due to error: {e}\")\n",
        "            continue\n",
        "\n",
        "        t1 = time.time()\n",
        "        latency = t1 - t0\n",
        "        peak = get_peak_vram_mib()\n",
        "\n",
        "        # Store results\n",
        "        results.append({\n",
        "            'image_id': i,\n",
        "            'references': [str(r) for r in refs] if refs is not None else [],\n",
        "            'pred': str(pred),\n",
        "            'latency_s': latency,\n",
        "            'peak_vram_mib': peak,\n",
        "        })\n",
        "\n",
        "    print(f\"Completed {len(results)} image evaluations.\")\n",
        "    return results\n",
        "\n",
        "print('Evaluation loop ready.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77eb94ae",
      "metadata": {
        "id": "77eb94ae"
      },
      "source": [
        "## 7 — CIDEr metric computation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65b6a47f",
      "metadata": {
        "id": "65b6a47f"
      },
      "outputs": [],
      "source": [
        "from pycocoevalcap.cider.cider import Cider\n",
        "\n",
        "def compute_cider(results):\n",
        "    gts = {i: [str(r) for r in res[\"references\"]] for i, res in enumerate(results)}\n",
        "    res = {i: [str(res[\"pred\"])] for i, res in enumerate(results)}\n",
        "\n",
        "    cider = Cider()\n",
        "    score, individual_scores = cider.compute_score(gts, res)\n",
        "\n",
        "    print(f\"CIDEr: {score:.4f}\")\n",
        "    return score"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1378768",
      "metadata": {
        "id": "e1378768"
      },
      "source": [
        "## 8 — Plotting (seaborn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9936ef8",
      "metadata": {
        "id": "c9936ef8"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "sns.set(style='whitegrid')\n",
        "\n",
        "def plot_comparison(fp16_metrics, int8_metrics, int8_vit_metrics, out_prefix='results/plot'):\n",
        "    # Convert to DataFrames and tag mode\n",
        "    df_fp = pd.DataFrame(fp16_metrics)\n",
        "    df_8 = pd.DataFrame(int8_metrics)\n",
        "    df_8_vit = pd.DataFrame(int8_vit_metrics)\n",
        "    df_fp['mode'] = 'fp16'\n",
        "    df_8['mode'] = 'int8'\n",
        "    df_8_vit['mode'] = 'vit_int8'\n",
        "    df = pd.concat([df_fp, df_8, df_8_vit], ignore_index=True)\n",
        "\n",
        "    # Ensure numeric columns are floats\n",
        "    df['latency_s'] = pd.to_numeric(df['latency_s'], errors='coerce')\n",
        "    df['peak_vram_mib'] = pd.to_numeric(df['peak_vram_mib'], errors='coerce')\n",
        "\n",
        "    # --- Boxplot for latency ---\n",
        "    plt.figure(figsize=(8,4))\n",
        "    ax = sns.boxplot(x='mode', y='latency_s', data=df)\n",
        "    ax.set_title('Latency per image (s)')\n",
        "    plt.savefig(out_prefix + '_latency.png', dpi=200)\n",
        "    plt.close()\n",
        "\n",
        "    # --- Boxplot for peak VRAM ---\n",
        "    plt.figure(figsize=(8,4))\n",
        "    ax = sns.boxplot(x='mode', y='peak_vram_mib', data=df)\n",
        "    ax.set_title('Peak VRAM per image (MiB)')\n",
        "    plt.savefig(out_prefix + '_vram.png', dpi=200)\n",
        "    plt.close()\n",
        "\n",
        "    print('Saved plots with prefix', out_prefix)\n",
        "\n",
        "print('Plotting utilities ready.')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_summary_metrics(summary, out_prefix='results/summary'):\n",
        "    metrics_info = {\n",
        "        'CIDEr': ('cider_fp16', 'cider_int8', 'cider_int8_vit'),\n",
        "        'Model Size (MB)': ('model_size_fp16_mb', 'model_size_int8_mb', 'model_size_int8_vit_mb'),\n",
        "        'Throughput (img/s)': ('throughput_fp16_img_s', 'throughput_int8_img_s', 'throughput_int8_vit_img_s'),\n",
        "        #'Energy (Wh)': ('energy_kwh_fp16', 'energy_kwh_int8', 'energy_kwh_int8_vit')\n",
        "    }\n",
        "\n",
        "    for metric_name, (fp16_key, int8_key, int8_vit_key) in metrics_info.items():\n",
        "        fp16_value = summary[fp16_key]\n",
        "        int8_value = summary[int8_key]\n",
        "        int8_vit_value = summary[int8_vit_key]\n",
        "\n",
        "        # Convert energy from kWh to Wh\n",
        "        if 'Energy' in metric_name:\n",
        "            fp16_value *= 1000\n",
        "            int8_value *= 1000\n",
        "            int8_vit_value *= 1000\n",
        "            fmt = '{:.7f}'\n",
        "        else:\n",
        "            fmt = '{:.3f}'\n",
        "\n",
        "        values = {\n",
        "            'Mode': ['FP16', 'INT8', 'INT8_VIT'],\n",
        "            'Value': [fp16_value, int8_value, int8_vit_value]\n",
        "        }\n",
        "        df = pd.DataFrame(values)\n",
        "\n",
        "        plt.figure(figsize=(5,4))\n",
        "        ax = sns.barplot(x='Mode', y='Value', hue='Mode', data=df, palette='pastel', dodge=False, legend=False)\n",
        "        ax.set_title(metric_name)\n",
        "\n",
        "        # Annotate with proper precision\n",
        "        for p in ax.patches:\n",
        "            ax.annotate(fmt.format(p.get_height()),\n",
        "                        (p.get_x() + p.get_width() / 2., p.get_height()),\n",
        "                        ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        safe_name = re.sub(r'[^A-Za-z0-9_]+', '_', metric_name)\n",
        "        file_path = f\"{out_prefix}_{safe_name}.png\"\n",
        "        plt.savefig(file_path, dpi=200)\n",
        "        plt.close()\n",
        "\n",
        "        print(f\"Saved plot for {metric_name}: {file_path}\")"
      ],
      "metadata": {
        "id": "yZEmv3vwBsg8"
      },
      "id": "yZEmv3vwBsg8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "ef5d69fe",
      "metadata": {
        "id": "ef5d69fe"
      },
      "source": [
        "## 9 — Orchestrator"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, csv, time, gc, torch\n",
        "from codecarbon import EmissionsTracker\n",
        "\n",
        "def run_inference_batch(\n",
        "    batch_id,\n",
        "    model_name,\n",
        "    pipe,\n",
        "    dataset,\n",
        "    batch_size,\n",
        "    exp_cfg,\n",
        "    img_field,\n",
        "    refs_field\n",
        "):\n",
        "    os.makedirs(\"results\", exist_ok=True)\n",
        "\n",
        "    # Calculate indices for this batch\n",
        "    start_idx = (batch_id - 1) * batch_size\n",
        "    end_idx = min(start_idx + batch_size, len(dataset))\n",
        "    csv_path = f\"results/{model_name.lower()}_batch_{batch_id:02d}.csv\"\n",
        "\n",
        "    # Skip if already exists\n",
        "    if os.path.exists(csv_path):\n",
        "        print(f\"Skipping batch {batch_id}: {csv_path} already exists.\")\n",
        "        return csv_path\n",
        "\n",
        "    print(f\"Running {model_name} batch {batch_id}: {start_idx}–{end_idx}\")\n",
        "\n",
        "    if hasattr(dataset, \"iloc\"):\n",
        "        # Pandas DataFrame\n",
        "        data_slice = dataset.iloc[start_idx:end_idx]\n",
        "    elif isinstance(dataset, list):\n",
        "        data_slice = dataset[start_idx:end_idx]\n",
        "    elif hasattr(dataset, \"select\"):\n",
        "        # Hugging Face Dataset\n",
        "        data_slice = dataset.select(range(start_idx, end_idx))\n",
        "    else:\n",
        "        raise TypeError(f\"Unsupported dataset type: {type(dataset)}\")\n",
        "\n",
        "    log_path = f\"results/{model_name.lower()}_n_samples_{len(data_slice)}_emissions.csv\"\n",
        "    tracker = EmissionsTracker(\n",
        "        project_name=f\"{model_name}_n_samples_{len(data_slice)}\",\n",
        "        output_dir=\"results\",\n",
        "        output_file=os.path.basename(log_path),\n",
        "        save_to_file=True\n",
        "    )\n",
        "    tracker.start()\n",
        "\n",
        "    # Inference loop\n",
        "    t0 = time.perf_counter()\n",
        "    batch_results = evaluate_full(\n",
        "        data_slice,\n",
        "        pipe,\n",
        "        img_field=img_field,\n",
        "        refs_field=refs_field,\n",
        "        max_items=None,\n",
        "        img_size=exp_cfg[\"img_size\"],\n",
        "        max_new_tokens=exp_cfg[\"max_new_tokens\"],\n",
        "    )\n",
        "    t1 = time.perf_counter()\n",
        "    tracker.stop()\n",
        "\n",
        "    # Save batch results\n",
        "    with open(csv_path, \"w\", newline=\"\") as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=[\"image_id\", \"references\", \"pred\", \"latency_s\", \"peak_vram_mib\"])\n",
        "        writer.writeheader()\n",
        "        writer.writerows(batch_results)\n",
        "\n",
        "    # Free memory\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    print(f\"Saved {len(batch_results)} results → {csv_path} ({t1 - t0:.1f}s)\")\n",
        "    return csv_path"
      ],
      "metadata": {
        "id": "sLYGIBAHLKUy"
      },
      "id": "sLYGIBAHLKUy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "63df6145",
      "metadata": {
        "id": "63df6145"
      },
      "source": [
        "## 10 — Run the experiment"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, gc, csv, time, torch, pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "exp_cfg = EXP[\"inference\"]\n",
        "model_id = EXP[\"model_id\"]\n",
        "limit = EXP[\"dataset\"][\"limit\"]\n",
        "batch_size = limit // 5  # divide dataset into 5 equal parts\n",
        "IMG_FIELD, REF_FIELD = \"coco_url\", \"captions\"\n",
        "\n",
        "os.makedirs(\"results\", exist_ok=True)\n",
        "print(f\"Setup complete. Processing {limit} images in 5 batches of {batch_size}.\")"
      ],
      "metadata": {
        "id": "kmeGXNMfrgFg"
      },
      "id": "kmeGXNMfrgFg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Building FP16 model …\")\n",
        "pipe_fp16 = build_fp16_model(model_id)\n",
        "fp16_size = get_model_size(pipe_fp16.model)\n",
        "print(f\"FP16 model size: {fp16_size:.1f} MB\")"
      ],
      "metadata": {
        "id": "CKwSBltNrcQZ"
      },
      "id": "CKwSBltNrcQZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_inference_batch(\n",
        "    batch_id=1,\n",
        "    model_name=\"FP16\",\n",
        "    pipe=pipe_fp16,\n",
        "    dataset=ds,\n",
        "    batch_size=batch_size,\n",
        "    exp_cfg=EXP[\"inference\"],\n",
        "    img_field=IMG_FIELD,\n",
        "    refs_field=REF_FIELD\n",
        ")"
      ],
      "metadata": {
        "id": "DcjKo-fbrj2L"
      },
      "id": "DcjKo-fbrj2L",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_inference_batch(\n",
        "    batch_id=2,\n",
        "    model_name=\"FP16\",\n",
        "    pipe=pipe_fp16,\n",
        "    dataset=ds,\n",
        "    batch_size=batch_size,\n",
        "    exp_cfg=EXP[\"inference\"],\n",
        "    img_field=IMG_FIELD,\n",
        "    refs_field=REF_FIELD\n",
        ")"
      ],
      "metadata": {
        "id": "K-o49hClKned"
      },
      "id": "K-o49hClKned",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_inference_batch(\n",
        "    batch_id=3,\n",
        "    model_name=\"FP16\",\n",
        "    pipe=pipe_fp16,\n",
        "    dataset=ds,\n",
        "    batch_size=batch_size,\n",
        "    exp_cfg=EXP[\"inference\"],\n",
        "    img_field=IMG_FIELD,\n",
        "    refs_field=REF_FIELD\n",
        ")"
      ],
      "metadata": {
        "id": "3ROidVTyrloo"
      },
      "id": "3ROidVTyrloo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_inference_batch(\n",
        "    batch_id=4,\n",
        "    model_name=\"FP16\",\n",
        "    pipe=pipe_fp16,\n",
        "    dataset=ds,\n",
        "    batch_size=batch_size,\n",
        "    exp_cfg=EXP[\"inference\"],\n",
        "    img_field=IMG_FIELD,\n",
        "    refs_field=REF_FIELD\n",
        ")"
      ],
      "metadata": {
        "id": "QPxecnmPLdhQ"
      },
      "id": "QPxecnmPLdhQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_inference_batch(\n",
        "    batch_id=5,\n",
        "    model_name=\"FP16\",\n",
        "    pipe=pipe_fp16,\n",
        "    dataset=ds,\n",
        "    batch_size=batch_size,\n",
        "    exp_cfg=EXP[\"inference\"],\n",
        "    img_field=IMG_FIELD,\n",
        "    refs_field=REF_FIELD\n",
        ")"
      ],
      "metadata": {
        "id": "sMxv1yP8LfJY"
      },
      "id": "sMxv1yP8LfJY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del pipe_fp16"
      ],
      "metadata": {
        "id": "8ylFsCvEOmOF"
      },
      "id": "8ylFsCvEOmOF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Building INT8 model …\")\n",
        "pipe_int8 = build_int8_model(model_id)\n",
        "int8_size = get_model_size(pipe_int8.model)\n",
        "print(f\"INT8 model size: {int8_size:.1f} MB\")"
      ],
      "metadata": {
        "id": "Vkb8Uh0qMu0q"
      },
      "id": "Vkb8Uh0qMu0q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_inference_batch(\n",
        "    batch_id=1,\n",
        "    model_name=\"INT8\",\n",
        "    pipe=pipe_int8,\n",
        "    dataset=ds,\n",
        "    batch_size=batch_size,\n",
        "    exp_cfg=EXP[\"inference\"],\n",
        "    img_field=IMG_FIELD,\n",
        "    refs_field=REF_FIELD\n",
        ")"
      ],
      "metadata": {
        "id": "rGSxzU1nN25P"
      },
      "id": "rGSxzU1nN25P",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_inference_batch(\n",
        "    batch_id=2,\n",
        "    model_name=\"INT8\",\n",
        "    pipe=pipe_int8,\n",
        "    dataset=ds,\n",
        "    batch_size=batch_size,\n",
        "    exp_cfg=EXP[\"inference\"],\n",
        "    img_field=IMG_FIELD,\n",
        "    refs_field=REF_FIELD\n",
        ")"
      ],
      "metadata": {
        "id": "NlThF4WTN730"
      },
      "id": "NlThF4WTN730",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_inference_batch(\n",
        "    batch_id=3,\n",
        "    model_name=\"INT8\",\n",
        "    pipe=pipe_int8,\n",
        "    dataset=ds,\n",
        "    batch_size=batch_size,\n",
        "    exp_cfg=EXP[\"inference\"],\n",
        "    img_field=IMG_FIELD,\n",
        "    refs_field=REF_FIELD\n",
        ")"
      ],
      "metadata": {
        "id": "C7PLtpPRN9iV"
      },
      "id": "C7PLtpPRN9iV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_inference_batch(\n",
        "    batch_id=4,\n",
        "    model_name=\"INT8\",\n",
        "    pipe=pipe_int8,\n",
        "    dataset=ds,\n",
        "    batch_size=batch_size,\n",
        "    exp_cfg=EXP[\"inference\"],\n",
        "    img_field=IMG_FIELD,\n",
        "    refs_field=REF_FIELD\n",
        ")"
      ],
      "metadata": {
        "id": "U3dNKE3pN_PX"
      },
      "id": "U3dNKE3pN_PX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_inference_batch(\n",
        "    batch_id=5,\n",
        "    model_name=\"INT8\",\n",
        "    pipe=pipe_int8,\n",
        "    dataset=ds,\n",
        "    batch_size=batch_size,\n",
        "    exp_cfg=EXP[\"inference\"],\n",
        "    img_field=IMG_FIELD,\n",
        "    refs_field=REF_FIELD\n",
        ")"
      ],
      "metadata": {
        "id": "bNizYWvJOAdq"
      },
      "id": "bNizYWvJOAdq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del pipe_int8"
      ],
      "metadata": {
        "id": "k9tbg7d3OqIC"
      },
      "id": "k9tbg7d3OqIC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Building INT8 VIT model …\")\n",
        "pipe_int8_vit = build_vit_int8_model(model_id)\n",
        "int8_vit_size = get_model_size(pipe_int8_vit.model)\n",
        "print(f\"INT8 VIT model size: {int8_vit_size:.1f} MB\")"
      ],
      "metadata": {
        "id": "0olRnbHGNn36"
      },
      "id": "0olRnbHGNn36",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_inference_batch(\n",
        "    batch_id=1,\n",
        "    model_name=\"INT8_VIT\",\n",
        "    pipe=pipe_int8_vit,\n",
        "    dataset=ds,\n",
        "    batch_size=batch_size,\n",
        "    exp_cfg=EXP[\"inference\"],\n",
        "    img_field=IMG_FIELD,\n",
        "    refs_field=REF_FIELD\n",
        ")"
      ],
      "metadata": {
        "id": "zrk1JR8qNmrq"
      },
      "id": "zrk1JR8qNmrq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_inference_batch(\n",
        "    batch_id=2,\n",
        "    model_name=\"INT8_VIT\",\n",
        "    pipe=pipe_int8_vit,\n",
        "    dataset=ds,\n",
        "    batch_size=batch_size,\n",
        "    exp_cfg=EXP[\"inference\"],\n",
        "    img_field=IMG_FIELD,\n",
        "    refs_field=REF_FIELD\n",
        ")"
      ],
      "metadata": {
        "id": "xb_CZFBxOIcN"
      },
      "id": "xb_CZFBxOIcN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_inference_batch(\n",
        "    batch_id=3,\n",
        "    model_name=\"INT8_VIT\",\n",
        "    pipe=pipe_int8_vit,\n",
        "    dataset=ds,\n",
        "    batch_size=batch_size,\n",
        "    exp_cfg=EXP[\"inference\"],\n",
        "    img_field=IMG_FIELD,\n",
        "    refs_field=REF_FIELD\n",
        ")"
      ],
      "metadata": {
        "id": "ef460H1fOKvz"
      },
      "id": "ef460H1fOKvz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_inference_batch(\n",
        "    batch_id=4,\n",
        "    model_name=\"INT8_VIT\",\n",
        "    pipe=pipe_int8_vit,\n",
        "    dataset=ds,\n",
        "    batch_size=batch_size,\n",
        "    exp_cfg=EXP[\"inference\"],\n",
        "    img_field=IMG_FIELD,\n",
        "    refs_field=REF_FIELD\n",
        ")"
      ],
      "metadata": {
        "id": "WY9OywgbOLx8"
      },
      "id": "WY9OywgbOLx8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_inference_batch(\n",
        "    batch_id=5,\n",
        "    model_name=\"INT8_VIT\",\n",
        "    pipe=pipe_int8_vit,\n",
        "    dataset=ds,\n",
        "    batch_size=batch_size,\n",
        "    exp_cfg=EXP[\"inference\"],\n",
        "    img_field=IMG_FIELD,\n",
        "    refs_field=REF_FIELD\n",
        ")"
      ],
      "metadata": {
        "id": "FHiQ1EWSONb4"
      },
      "id": "FHiQ1EWSONb4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del pipe_int8_vit"
      ],
      "metadata": {
        "id": "9fhFlwzdOuKu"
      },
      "id": "9fhFlwzdOuKu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10 — Compute metrics and compare"
      ],
      "metadata": {
        "id": "aEw9Mz9ernjg"
      },
      "id": "aEw9Mz9ernjg"
    },
    {
      "cell_type": "code",
      "source": [
        "import glob, ast, re, pandas as pd\n",
        "\n",
        "def merge_batches(prefix):\n",
        "    files = sorted(glob.glob(f\"results/{prefix}_batch_*.csv\"))\n",
        "    if not files:\n",
        "        raise FileNotFoundError(f\"No batch files found for prefix '{prefix}'\")\n",
        "\n",
        "    print(f\"Merging {len(files)} batch files for '{prefix}'...\")\n",
        "    dfs = [pd.read_csv(f) for f in files]\n",
        "    df = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "    # Clean up references\n",
        "    def parse_refs(ref):\n",
        "        if isinstance(ref, str):\n",
        "            try:\n",
        "                parsed = ast.literal_eval(ref)\n",
        "                if isinstance(parsed, list):\n",
        "                    return [str(x) for x in parsed]\n",
        "            except Exception:\n",
        "                return [ref]\n",
        "        return [str(ref)]\n",
        "\n",
        "    df[\"references\"] = df[\"references\"].apply(parse_refs)\n",
        "\n",
        "    avg_latency = df[\"latency_s\"].mean()\n",
        "    throughput = len(df) / df[\"latency_s\"].sum()\n",
        "\n",
        "    print(f\"Merged {len(df)} results. Avg latency: {avg_latency:.2f}s, throughput: {throughput:.3f} img/s\")\n",
        "\n",
        "    return {\n",
        "        \"results\": df.to_dict(orient=\"records\"),\n",
        "        \"avg_latency_s\": avg_latency,\n",
        "        \"throughput_img_s\": throughput\n",
        "    }"
      ],
      "metadata": {
        "id": "ulq-V0b2Lofu"
      },
      "id": "ulq-V0b2Lofu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fp16_data = merge_batches(\"fp16\")\n",
        "print(f\"Merged FP16 results → {len(fp16_data['results'])} samples\")"
      ],
      "metadata": {
        "id": "WaH_LK56L4Lv"
      },
      "id": "WaH_LK56L4Lv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "int8_data = merge_batches(\"int8\")\n",
        "print(f\"Merged INT8 results → {len(int8_data['results'])} samples\")"
      ],
      "metadata": {
        "id": "NBIxamFsL5V8"
      },
      "id": "NBIxamFsL5V8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "int8_vit_data = merge_batches(\"int8_vit\")\n",
        "print(f\"Merged INT8 VIT results → {len(int8_vit_data['results'])} samples\")"
      ],
      "metadata": {
        "id": "PU3QS8e_L510"
      },
      "id": "PU3QS8e_L510",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Computing CIDEr scores...\")\n",
        "cider_fp16 = compute_cider(fp16_data[\"results\"])\n",
        "cider_int8 = compute_cider(int8_data[\"results\"])\n",
        "cider_int8_vit = compute_cider(int8_vit_data[\"results\"])\n",
        "\n",
        "print(f\"CIDEr FP16: {cider_fp16:.3f} | INT8: {cider_int8:.3f} | INT8-VIT: {cider_int8_vit:.3f}\")\n",
        "\n",
        "plot_comparison(fp16_data[\"results\"], int8_data[\"results\"], int8_vit_data[\"results\"])\n",
        "print(\"Comparison plot generated.\")\n",
        "\n",
        "summary = {\n",
        "    \"cider_fp16\": cider_fp16,\n",
        "    \"cider_int8\": cider_int8,\n",
        "    \"cider_int8_vit\": cider_int8_vit,\n",
        "    \"delta_cider_int8\": cider_int8 - cider_fp16,\n",
        "    \"delta_cider_int8_vit\": cider_int8_vit - cider_fp16,\n",
        "    \"avg_latency_fp16_s\": fp16_data[\"avg_latency_s\"],\n",
        "    \"avg_latency_int8_s\": int8_data[\"avg_latency_s\"],\n",
        "    \"avg_latency_int8_vit_s\": int8_vit_data[\"avg_latency_s\"],\n",
        "    \"throughput_fp16_img_s\": fp16_data[\"throughput_img_s\"],\n",
        "    \"throughput_int8_img_s\": int8_data[\"throughput_img_s\"],\n",
        "    \"throughput_int8_vit_img_s\": int8_vit_data[\"throughput_img_s\"],\n",
        "    \"model_size_fp16_mb\": fp16_size,\n",
        "    \"model_size_int8_mb\": int8_size,\n",
        "    \"model_size_int8_vit_mb\": int8_vit_size,\n",
        "    \"n_samples\": len(fp16_data[\"results\"]),\n",
        "    \"timestamp\": datetime.utcnow().isoformat() + \"Z\"\n",
        "}\n",
        "\n",
        "plot_summary_metrics(summary)\n",
        "\n",
        "summary_path = f\"results/summary_{datetime.utcnow().strftime('%Y%m%dT%H%M%SZ')}.json\"\n",
        "with open(summary_path, \"w\") as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "print(f\"Summary saved to: {summary_path}\")"
      ],
      "metadata": {
        "id": "LDD7IIH_rnN9"
      },
      "id": "LDD7IIH_rnN9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Captioning Examples"
      ],
      "metadata": {
        "id": "7ZnDLEPK6-Rv"
      },
      "id": "7ZnDLEPK6-Rv"
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def show_random_captions(pipe, dataset, img_field, refs_field, n_samples=3):\n",
        "    for _ in range(n_samples):\n",
        "        idx = random.randint(0, len(dataset) - 1)\n",
        "        row = dataset[idx]\n",
        "\n",
        "        image_path = row.get(img_field) if isinstance(row, dict) else row[img_field]\n",
        "        img = load_image(image_path)\n",
        "        plt.imshow(img)\n",
        "        plt.axis(\"off\")\n",
        "        plt.title(f\"Image {idx}\")\n",
        "        plt.show()\n",
        "\n",
        "        references = row.get(refs_field, [])\n",
        "        if isinstance(references, str):\n",
        "            try:\n",
        "                references = eval(references)\n",
        "            except Exception:\n",
        "                references = [references]\n",
        "\n",
        "        try:\n",
        "            pred = generate_caption_with_model(pipe, img).lower().split()\n",
        "            print(f\"Predicted Caption:\\n{' '.join(pred)}\\n\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating caption: {e}\")\n",
        "            continue\n",
        "\n",
        "        print(f\"References: {references}\\n\")"
      ],
      "metadata": {
        "id": "XafMGVsN7lca"
      },
      "id": "XafMGVsN7lca",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FP16 Model"
      ],
      "metadata": {
        "id": "0T3ADuXi7Nit"
      },
      "id": "0T3ADuXi7Nit"
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    pipe_fp16\n",
        "except NameError:\n",
        "    pipe_fp16 = build_fp16_model(EXP[\"model_id\"])\n",
        "show_random_captions(pipe_fp16, ds, IMG_FIELD, REF_FIELD)"
      ],
      "metadata": {
        "id": "hcddPdWR55py"
      },
      "id": "hcddPdWR55py",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### INT8"
      ],
      "metadata": {
        "id": "CI4PTPyW7bZo"
      },
      "id": "CI4PTPyW7bZo"
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    pipe_int8\n",
        "except NameError:\n",
        "    pipe_int8 = build_int8_model(EXP[\"model_id\"])\n",
        "show_random_captions(pipe_int8, ds, IMG_FIELD, REF_FIELD)"
      ],
      "metadata": {
        "id": "BHDd6YbY7clf"
      },
      "id": "BHDd6YbY7clf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### INT8_VIT"
      ],
      "metadata": {
        "id": "mAZueNp-7c_n"
      },
      "id": "mAZueNp-7c_n"
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    pipe_int8_vit\n",
        "except NameError:\n",
        "    pipe_int8_vit = build_vit_int8_model(EXP[\"model_id\"])\n",
        "show_random_captions(pipe_int8_vit, ds, IMG_FIELD, REF_FIELD)"
      ],
      "metadata": {
        "id": "9G_34IFA7epO"
      },
      "id": "9G_34IFA7epO",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}